{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mirandasaari1/Karpathy-Tensor-Flow-Conversion/blob/master/Project_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ars2G0tkkF_X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Translating a Recurrent Neural Network (RNN) model developed in Python to Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "SnV7peD6kJ5h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Created By Rosa Garza and Miranda Saari"
      ]
    },
    {
      "metadata": {
        "id": "gztPo7SrkN5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ]
    },
    {
      "metadata": {
        "id": "L0R8pVEZkQRD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following code is a tensorflow interpretation of Andrej Karpathy's python code. The python code can be found at [Karpathy's python code](https://gist.github.com/karpathy/d4dee566867f8291f086/)"
      ]
    },
    {
      "metadata": {
        "id": "P4lJlwhC_rij",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenges"
      ]
    },
    {
      "metadata": {
        "id": "6KqHChPr_tQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "  Throughout this project we encountered a variety of different challenges, mostly stemming from our unfamiliarity with tensorflow and RNNs. We would say the first major challenge was developing a clear understanding of Karpathy's code and translating it to Tensorflow. When beginning to look at Karpathy’s code, it was very overwhelming, especially when seeing all of the linear algebra being performed within the loss function. As a first step, we decided to look at the MNIST RNN model and base our program off of its structure.\n",
        "\n",
        "One of the first issues we faced before getting our program to run was figuring out how we were going to input our data into the RNN. Because we knew based on Karpathy’s code, encoding needed to be done, we had to decide which method would be more efficient: encoding data before being sent through tensorflow or after. We learned by talking to Dr. Bruns that the more efficient method was to one hot encode the data in tensorflow all at once. After having some troubles attempting to encode all at one time we found a way to one hot encode the data one batch at a time which was our chosen method. Since our fetch batch was creating a numpy array of indices which corresponds to characters and there was a tensorflow function to encode the data, we decided to encode our input after sending through our RNN model. With the encoding of the 2D numpy array, it then changed the dimensions of our X input value and caused some issues in our thinking of how the RNN model was being trained. Having to visualize the training on the 3D data caused us a lot of confusion about how our program was running.\n",
        "\n",
        " After the data was encoded we came across multiple issues in regards to how we the data was being input. An error we faced was involving data types such as the input of our data was in terms of floats and our output value was in terms of integers. In order to not raise an error, we changed both our input and output values to be of type int.\n",
        " \n",
        "By starting our RNN model off of the MNIST RNN model, we realized we weren’t understanding why certain Tensorflow functions were being used. For example, the MNIST RNN model didn’t use the OutputProjectionWrapper() or dynamic_rnn() which caused us to look at other sources, such as our course book’s RNN model and other online github open source code. Because we didn’t have a clear understanding of why certain functions were being used, this led to us having various functions in our RNN model that either weren’t needed or being used correctly. With various functions occurring throughout the program, we received weird errors such as “Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2)”. We then decided to change various dimensions being put through Tensorflow which would result in more errors, specifically of the  dimensions of our input and output data.\n",
        "\n",
        " After having trouble understanding the dimensionality of our input and output values, when then scheduled a meeting with Dr. Bruns which helped tremendously. Verbally talking with Dr. Bruns and seeing an overall picture of Karpathy’s method helped us to have a more clear understanding of how our program should be sending in input and outputting data. Through discussion with Dr. Bruns of the reasoning behind an OutputProjectionWrapper() allowed us to have a clearer understanding of why using the function helped in terms of training and having the correct number of outputs. It also helped us to learn the reasoning behind why programs have a dense layer in which we now know moving forward to really understand the importance of taking the time to comprehend how Tensorflow functions work, how they are being used in the program, and why they are needed.\n",
        "Knowing our RNN model was going to calculate our loss for us and we weren’t going to have a loss function like Karpathy, we mistakenly overlooked the Karpathy’s loss function and focused on finding resources which would help to get our RNN model to work. This led to us having issues with our RNN model’s dimensions not being correct within the training and causing our program to crash. After meeting with Dr. Bruns, we realized the importance of understanding Karpathy’s loss function and how large of an impact it had being with helping to figure out what calculation is occurring throughout each step of the neural network. From this mistake, it forced us to really know the dimensions of each input and output from the RNN’s functions.  By knowing the dimensions after each training step of the RNN, it helped us be able to debug our program through the print out of certain variables’ shapes. This also helped show where in our model were we using certain variables incorrectly. Moving forward in the development of any future neural network program, we now realize the importance of fully understanding tensorflow functions, why they are being used, and what is being output.\n",
        "\n",
        "Once all of our many complex challenges were solved, we were able to output the loss as we ran the RNN, which was our first big accomplishment. From here we began to look into finding methods for how to build the array of indices to indicate the character prediction. Trying to understand Karpathy's methods of how character output was implemented while considering our own RNN implementation proved to be difficult.For example, it was difficult to understand which part of Karpathy's code was still needed to be implemented and which part of our RNN was already implementing certain tasks for us. In our RNN after implementing a softmax function we found a vector of probabilities which was returned to us, however, we were unsure about what to do with these probabilities. We then talked to Dr. Bruns about the best way to utilize this vector of probabilities which was of dimension (batch size, 25,69) for character selection. With his advisement we chose from the vector which was in the first row and last column of the probability vector. Lastly we created the list of 200 indices such as karpathy to find the characters which would be printed with every 5 epochs. Seeing our very first output was a huge accomplishment for us and from there we were able to build our second model.\n",
        "\n",
        "After seeing the output from our RNN model, we did have doubts about whether or not the program was training and making predictions correctly. Once we developed our second model and changed certain parameters, with more training we realized our model was actually improving and training correctly. The very first outputs resembled gibberish, but at the end after training was finished, we noticed words were being formed and punctuation was correct. Seeing the program overtime begin to form words such as “I’m”, “Alice”, and “Oh,” was very exciting. Now we feel very accomplished with being able to build a neural network model that can predict text. Moving forward, from this experience, we now have good practice skills for developing future machine learning models and know the important steps required to building successful models."
      ]
    },
    {
      "metadata": {
        "id": "wIeEvHa4kS9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1"
      ]
    },
    {
      "metadata": {
        "id": "WYfuvZ24OF3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For part 1 of our project we translated Karpathy's python code to TensorFlow. Throughout the following code we will talk about what stayed the same as Karpathy and what changed."
      ]
    },
    {
      "metadata": {
        "id": "1O5FPaTXlRXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ]
    },
    {
      "metadata": {
        "id": "splenSW7jpDP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-GR0ufZFlfAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Alice Data"
      ]
    },
    {
      "metadata": {
        "id": "e1I3MFwn9nRW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following is from Karapthy's original code in which he creates the necessary variables and arrays for text translation."
      ]
    },
    {
      "metadata": {
        "id": "ErTDJ7pRliwk",
        "colab_type": "code",
        "outputId": "1be4a6fb-80d3-450a-c900-694a176d7302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "data = open('alice.txt', 'r').read() # should be simple plain text file\n",
        "#gets a unique list of characters in the file\n",
        "chars = list(set(data))\n",
        "\n",
        "#number of unique characters\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "#creates an index for each character to be correlated to\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data has 144393 characters, 69 unique.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "75oBvsTLHcni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "kLgzor72RbyK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following stayed the same as Karpathy's code: sequence length and the number of neurons in a cell.\n",
        "Since Karpathy did not have a defined number of epochs we chose to start with 1000 to see how our model performed and go up from there. "
      ]
    },
    {
      "metadata": {
        "id": "yNYyfiJCHfsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_hidden     = 100 # size of hidden layer of neurons\n",
        "num_epochs     = 4000\n",
        "learning_rate  = 0.001\n",
        "batch_size     = 200 # sample length, similar to Karpathy \n",
        "training_steps = 5\n",
        "timesteps      = 25 # number of moments of time for training and sending in an \"input value\" to the network\n",
        "seq_length     = 25 # number of steps to unroll the RNN for\n",
        "num_classes    = len(char_to_ix) # Number of possible characters, the length of array after encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "109kfGtQI_hf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "smOIfkh6Iu6t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below was a suggestion from Dr. Bruns of how to preprocess our initial array 'data' of text."
      ]
    },
    {
      "metadata": {
        "id": "pj7NaQ3nIp3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create training sequences and corresponding labels\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0, len(data)-seq_length-1, 1):\n",
        "        X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
        "        y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
        "# reshape the data\n",
        "# in X_modified, each row is an encoded sequence of characters\n",
        "X_modified = np.reshape(X, (len(X), seq_length))\n",
        "# in Y modified, each row corresponds to X_modified, but each row's value is\n",
        "# offset by 1 in comparison to X_mod.\n",
        "y_modified = np.reshape(y, (len(y), seq_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YSfPHoTNKo44",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Building RNN"
      ]
    },
    {
      "metadata": {
        "id": "XVQ5H7gWlQxu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbiZGm7sT0Uz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because Karpathy trained the Python RNN model on an encoding of the integer values in each array of text, our chosen method of input for the RNN model was to send in a numpy array of length 25, and then use tensorflow to encode each value.  We knew based on Karpathy's code we were sending in a sequence and outputing a sequence, therefore our output was going to be the length of the sequence."
      ]
    },
    {
      "metadata": {
        "id": "AAnGHjEVKqy5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "############################### CONSTRUCTION PHASE ##############################\n",
        "X          = tf.placeholder(tf.int32, [None, timesteps],name='Input_batch') # (batch_size, 25)\n",
        "encoded    = tf.one_hot(X,depth=num_classes) # encodes each row to get: (batch_size, 25, 69)\n",
        "Y          = tf.placeholder(tf.int32, [None,timesteps], name=\"Output\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4HV6Z5KBWVzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is our development of our RNN model and preparing for the training input."
      ]
    },
    {
      "metadata": {
        "id": "p7xrtFcUId6X",
        "colab_type": "code",
        "outputId": "d3df16c5-6dd1-4b35-a163-2a4a060e1c4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "cell    = tf.contrib.rnn.OutputProjectionWrapper(\n",
        "        tf.contrib.rnn.BasicRNNCell(num_units = num_hidden,activation = tf.nn.tanh),\n",
        "        output_size = num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-cb27b5e5e8cb>:2: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B-jaDCc3XS4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the lines below, we are performing the first four lines of the first for loop within the loss function of Karpathy's Python code. We are recieving \"outputs\", which relative to Karpathy's \"ys\" dictionary and\"states\", which is relative to Karpathy's \"hs\" dictionary."
      ]
    },
    {
      "metadata": {
        "id": "jptBaz_9IktU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outputs, states = tf.nn.dynamic_rnn(cell, encoded, dtype=tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZjtKTYmZJy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In Karpathy's RNN Python code,  backpropagation was implemented from scratch in which we did not do. Instead we used an AdamOptimizer where we needed to recieve probabilities on our multi classification problem and reduce the error. As we were training on the program, we continued to make sure we reduced the loss."
      ]
    },
    {
      "metadata": {
        "id": "gv97OHGjInDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = outputs,name=\"xentropy\")\n",
        "loss        = tf.reduce_mean(xentropy,name=\"loss\")\n",
        "optimizer   = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "training_op = optimizer.minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qfzcdHTFYu9L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The code below is the fifth of the first for loop within the loss function of Karpathy's Python code. This allowed us to get the probabilities of our outputs."
      ]
    },
    {
      "metadata": {
        "id": "CjYIPZlQIp93",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "probabilities = tf.nn.softmax(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q6D2JYz4ar1t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is where we are able to see the correct predictions and accuracy score of our RNN model."
      ]
    },
    {
      "metadata": {
        "id": "vMrXeqEMYhGo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########### NEEDS TO BE RERAN...SHOULD WORK ##########\n",
        "correct     = tf.equal(tf.argmax(outputs, 1), tf.argmax(Y, 1))\n",
        "accuracy    = tf.reduce_mean(tf.cast(correct, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLu6QEzuIxQ7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K25xLjbgmWnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Producing Mini Batches"
      ]
    },
    {
      "metadata": {
        "id": "ng0DvHr8bpyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below is how we fetch the mini batches of our text data in order to train with our RNN model."
      ]
    },
    {
      "metadata": {
        "id": "fAFITZ_wmeO0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### FETCH BATCH ####\n",
        "def fetch_batch(epoch, batch_size):\n",
        "    np.random.seed(epoch * batch_size)\n",
        "    indices = np.random.randint(len(X_modified), size=batch_size)\n",
        "    X_batch = X_modified[indices] #(batch_size,25)\n",
        "    Y_batch = y_modified[indices]\n",
        "    return X_batch, Y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aZZuc1fXmlHJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Excecution"
      ]
    },
    {
      "metadata": {
        "id": "jH94RzZVc6QC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the execution phase of our RNN model. In the inner for loop is where we are running our program similar to Karpathy's sample function. We print out the epoch, which in Karpathy's is the iteration, and the loss. Our code then continuously uses the first array of our mini batch which is similar to Karpathy's code within the while loop: sample_ix = sample(hprev, inputs[0], 200).\n",
        "\n",
        "In Karpathy's Python program, he sends the first value of inputs to his sample function and encodes it in order to have the model predict the following index. The way Karpathy's sample method works is that he continue to do predictions for the remaining values in the sequence list, but the next prediction index is based on sampling of the probabilities list.  This implementation is the from the following code: pred_index=np.random.choice(range(len(probs[0][-1])), p=probs[0][-1].ravel()).\n",
        "\n",
        "After continuing to save the predicted indices, we then translated the list into characters and output our predicted text."
      ]
    },
    {
      "metadata": {
        "id": "BD_HIWZWmmWI",
        "colab_type": "code",
        "outputId": "2883e483-df4c-443a-8f76-591fac7263b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1666
        }
      },
      "cell_type": "code",
      "source": [
        "########################## EXECUTION PHASE ##############################\n",
        "with tf.Session() as sess:\n",
        "    init.run() \n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        x_batch,y_batch = fetch_batch(epoch,batch_size)\n",
        "        sess.run(training_op, feed_dict={X: x_batch, Y: y_batch})\n",
        "        loss_val=loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
        "        if epoch%20==0:\n",
        "            print ('Epoch: ', epoch,\"Loss: \",loss_val)\n",
        "            predictions=[]\n",
        "            x_temp=np.reshape(x_batch[0],(1,-1))\n",
        "            #sampling \n",
        "            for i in range(200):\n",
        "                probs=probabilities.eval(feed_dict={X: x_temp})\n",
        "                pred_index=np.random.choice(range(len(probs[0][-1])), p=probs[0][-1].ravel())\n",
        "                predictions.append(pred_index)\n",
        "                x_temp=np.reshape(np.append(x_temp,pred_index)[1:],(1,-1))\n",
        "            txt = ''.join(ix_to_char[ix] for ix in predictions)\n",
        "            print ('----\\n %s \\n----' % (txt, ))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  20 Loss:  3.3510735\n",
            "----\n",
            " hd-, endeo?,!l QfJrduks  s eeaiDeUvk hhgtcNuv eeouDd mgb,erD(opih nn rrfh ,ewpdoihqFhukeDK-e  d nidnmvemfrslsNaueMdaeJheI w ip slose' seYn 'edTawpyheb k lejs ki esueeobe J r ,a C hesaoeiCdhwk onl e ae \n",
            "----\n",
            "Epoch:  40 Loss:  3.1767948\n",
            "----\n",
            " oseyiu\n",
            " ehs\n",
            "ityd od Ma  l iato''ittpe le  e yrlrfoeb oTta oIsIyna !hllmr oii ycsi H t'w t inee tuhon!teti'lesr]oln olsoaom t l  lcR, aalDs,rkitna\n",
            "ksuigCr- endetcervs,s e-od n   -dr:eee o  sh si  nehei \n",
            "----\n",
            "Epoch:  60 Loss:  3.0921996\n",
            "----\n",
            " g s rirMtnk k a d\n",
            "w  haa ulmen  s  sen'iigs  fi p tIiolwsltnd\n",
            "tenesases,uihegis tlst  hsveb'ynls  hot rlr scdtinttohh_y , p ildtnshndrscisuesAede  ngiiWhesoeaeir.e rtaO vrie os g   s uihea'ege wcsdhdn \n",
            "----\n",
            "Epoch:  80 Loss:  3.0234797\n",
            "----\n",
            " e,me lsEus si d Edetpt cW \"ieu'yohans orreno\n",
            "a s'asqaOxn yhi ,oto; r lr e es merke\"re  rotff ae,Harsefe riouahn kia eitmo  moisuepe' hed  t Dtahtcpit oyotpuusaoguagshtneosartED'te ge ewust ncewhaann   \n",
            "----\n",
            "Epoch:  100 Loss:  2.9267733\n",
            "----\n",
            " tpQsaqpe\n",
            "yogp euneMayP tsb a.Rthi deuye d iol \n",
            "aimmnRki' y) pi ioafCoaelte aog of tulie;yahs taelvancsiif  b,ibDtYh' ghwdtp hkgtsrrr Flet\n",
            "' n  anelrohe bee' f ie asrentn( ovcgn'; buule 'inklt eh gdtdw \n",
            "----\n",
            "Epoch:  120 Loss:  2.8443005\n",
            "----\n",
            "  augliagaglle ke Har hDksne te  ec oir he,lIeg lh\n",
            "tta \n",
            "w'ikunOoDaurlmoorwBse  lts,-ya;dar esoind wee tei n  h n, w ,ek,t Uven soes tnns\n",
            "hnZ -\n",
            "Taani m aa eMe whe ho\n",
            "t Irotiuei' ghot  on ahd  enltaau w' \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1122096f03fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "-nMJjOxvz1rq",
        "colab_type": "code",
        "outputId": "2e54e09d-c2f7-4456-81f3-5f036000271f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run() \n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        x_batch,y_batch = fetch_batch(epoch,batch_size)\n",
        "        sess.run(training_op, feed_dict={X: x_batch, Y: y_batch})\n",
        "        loss_val=loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
        "    print (\"Loss: \",loss_val)\n",
        "    predictions=[]\n",
        "    x_temp=np.reshape(x_batch[0],(1,-1))\n",
        "    #sampling \n",
        "    for i in range(200):\n",
        "      probs=probabilities.eval(feed_dict={X: x_temp})\n",
        "      pred_index=np.random.choice(range(len(probs[0][-1])), p=probs[0][-1].ravel())\n",
        "      predictions.append(pred_index)\n",
        "      x_temp=np.reshape(np.append(x_temp,pred_index)[1:],(1,-1))\n",
        "    txt = ''.join(ix_to_char[ix] for ix in predictions)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  1.582048\n",
            "----\n",
            " p a lotkly pleasces,' lare of herrystares\n",
            "so to--quite oonning hold, it's kel makings!' Alise herryouid!' sardblst\n",
            "about: notsefid itmesxiould,' said Alice.\n",
            "\n",
            "Alice to one, you know the sholked twintw, \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gwEa741SnBCa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2"
      ]
    },
    {
      "metadata": {
        "id": "nI0i0utwT8lp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For part 2 we decided to try different combinations of the model's hyperparameters to see if we could get reach better results. These changes included: growing the batch size,  timesteps, and sequence length. Although this increased training time, we wanted to see if our results would improve."
      ]
    },
    {
      "metadata": {
        "id": "BnFgE_gXgYJV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_hidden     = 100 # size of hidden layer of neurons\n",
        "num_epochs     = 4000\n",
        "learning_rate  = 0.001\n",
        "batch_size     = 400 # changed \n",
        "training_steps = 5\n",
        "timesteps      = 40 # changed\n",
        "seq_length     = 40 # changed\n",
        "num_classes    = len(char_to_ix) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s66rE0lFnvR8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following code is like that of above, as for this model we only changed hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "HF4j3wcigqX4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create training sequences and corresponding labels\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0, len(data)-seq_length-1, 1):\n",
        "        X.append([char_to_ix[ch] for ch in data[i:i+seq_length]])\n",
        "        y.append([char_to_ix[ch] for ch in data[i+1:i+seq_length+1]])\n",
        "X_modified = np.reshape(X, (len(X), seq_length))\n",
        "y_modified = np.reshape(y, (len(y), seq_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5KhcXUUgxlJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSOAX1g7hohU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X          = tf.placeholder(tf.int32, [None, timesteps],name='Input_batch') # (batch_size, 25)\n",
        "encoded    = tf.one_hot(X,depth=num_classes) # encodes each row to get: (batch_size, 25, 69)\n",
        "Y          = tf.placeholder(tf.int32, [None,timesteps],name=\"Output\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5wups4D_jMRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cell    = tf.contrib.rnn.OutputProjectionWrapper(\n",
        "        tf.contrib.rnn.BasicRNNCell(num_units = num_hidden,activation = tf.nn.tanh),\n",
        "        output_size = num_classes)\n",
        "outputs, states = tf.nn.dynamic_rnn(cell, encoded, dtype=tf.float32)\n",
        "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = Y, logits = outputs,name=\"xentropy\")\n",
        "loss        = tf.reduce_mean(xentropy,name=\"loss\")\n",
        "optimizer   = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
        "training_op = optimizer.minimize(loss)\n",
        "probabilities = tf.nn.softmax(outputs) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "umJv1nsHhJMB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf2d5m2gn7DZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After running the second model for the same number of epochs we saw a decrease in loss much quicker than that of the first model."
      ]
    },
    {
      "metadata": {
        "id": "fY67yHpMg4w2",
        "colab_type": "code",
        "outputId": "f07093a8-b6bb-4c97-b0fc-43d3fc4a6d71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1648
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run() \n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        x_batch,y_batch = fetch_batch(epoch,batch_size)\n",
        "        sess.run(training_op, feed_dict={X: x_batch, Y: y_batch})\n",
        "        loss_val=loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
        "        if epoch%20==0:\n",
        "            print ('Epoch: ', epoch,\"Loss: \",loss_val)\n",
        "            predictions=[]\n",
        "            x_temp=np.reshape(x_batch[0],(1,-1))\n",
        "            #sampling \n",
        "            for i in range(200):\n",
        "                probs=probabilities.eval(feed_dict={X: x_temp})\n",
        "                pred_index=np.random.choice(range(len(probs[0][-1])), p=probs[0][-1].ravel())\n",
        "                predictions.append(pred_index)\n",
        "                x_temp=np.reshape(np.append(x_temp,pred_index)[1:],(1,-1))\n",
        "            txt = ''.join(ix_to_char[ix] for ix in predictions)\n",
        "            print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  20 Loss:  3.2930088\n",
            "----\n",
            " h gi  pehed n o R!ttuntHUnoreeeoCroloQo-ltreTlho)Seaietnmr utnohmhiw e v aW\n",
            " d'; WsoitIo iW'wo sogKeeel,[rIdhn-M Vs ,as hkhlns tmia ajdmtt   :\n",
            "hwo\n",
            " stss e  tLr'zu oneabw y \n",
            "\n",
            "d feu Fh u? ihy\n",
            "(e'M'tA\n",
            "_T \n",
            "----\n",
            "Epoch:  40 Loss:  3.1733181\n",
            "----\n",
            " eo'te g yeuurt n e\n",
            "_aannm e   te\n",
            "omhde  a-kaeek ' \n",
            "hsTuoxgohagtnsre i sn . fond ssuer-'af nt   ,o si.e,elrf al omu ktnd rrwsye\n",
            "hf'ts  ee.hB l t' Uatntaclaw,to dgleied  m  ahi oleshenld'tucrae laognlh, \n",
            "----\n",
            "Epoch:  60 Loss:  3.1233652\n",
            "----\n",
            " wed  rder  ep'phu   nataa\n",
            " \n",
            "r .ag  o n  ua-edlthr iun o\n",
            "nh ensaniven w detr\n",
            " ;aeh n:ihr s l  Aus gdee\n",
            "eoaelory t ins,gee Tc una \"ht \n",
            "ig  aihir Beho wlr'anmsnwrec uesehadrue te  tgcp'hehiortitwunuae \n",
            "s \n",
            "----\n",
            "Epoch:  80 Loss:  3.0606353\n",
            "----\n",
            " fNfolwg h'''e ekevn pttt   pmeo tuilsihwoilde etn'ike\n",
            "wi!uicce d eld aeiudvssaf,run ofoicdf dee teg to  ihrVahodf tein aul iarg .ti a\n",
            "'unin efhiwaekdeha Iihuhvfe dmrnorl souerda  H mue o ecI\n",
            "e o. llda \n",
            "----\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1122096f03fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "EmVANNov0iZG",
        "colab_type": "code",
        "outputId": "74c8255f-f679-4cbe-e378-e9190a328d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    init.run() \n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        x_batch,y_batch = fetch_batch(epoch,batch_size)\n",
        "        sess.run(training_op, feed_dict={X: x_batch, Y: y_batch})\n",
        "        loss_val=loss.eval(feed_dict={X: x_batch, Y: y_batch})\n",
        "    print (\"Loss: \",loss_val)\n",
        "    predictions=[]\n",
        "    x_temp=np.reshape(x_batch[0],(1,-1))\n",
        "    #sampling \n",
        "    for i in range(200):\n",
        "      probs=probabilities.eval(feed_dict={X: x_temp})\n",
        "      pred_index=np.random.choice(range(len(probs[0][-1])), p=probs[0][-1].ravel())\n",
        "      predictions.append(pred_index)\n",
        "      x_temp=np.reshape(np.append(x_temp,pred_index)[1:],(1,-1))\n",
        "    txt = ''.join(ix_to_char[ix] for ix in predictions)\n",
        "    print ('----\\n %s \\n----' % (txt, ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss:  1.4670002\n",
            "----\n",
            " Lor OUR said Alice; 'Of\n",
            "charsels, caring?' Alice,' and had seand the Duchess makes, and Gryphon, and hel putsing it was lookhang, chanst\n",
            "maves--'\n",
            "\n",
            "'I' lef 'No--ad you her myone, and walket into reelin \n",
            "----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9bPqv38CnEk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "hlrIr072oZfD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After many hours,  trying different approaches, trial and error; we are happy with our implementation of Karpathy's python code. We are sure the longer we allow our models to run the better results they would produce, such as clearer words and more complete sentences. In just 1000 epochs, we saw a significant drop in our loss from around 3.4 to around 1.9. Changing the epochs to 2000 we saw the loss drop as low as 1.70 with the first model and 1.64 in the second model, thus we can imagine as the model trains longer, the results will only improve. Overall, this project was a new challenge we had not yet experienced, and looking back we are glad we had a chance to work on it."
      ]
    }
  ]
}